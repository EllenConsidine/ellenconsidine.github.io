<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Artificial Intelligence | Ellen Considine</title>
    <link>http://localhost:4321/tags/artificial-intelligence/</link>
      <atom:link href="http://localhost:4321/tags/artificial-intelligence/index.xml" rel="self" type="application/rss+xml" />
    <description>Artificial Intelligence</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Sun, 10 Mar 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:4321/media/icon_hu68170e94a17a2a43d6dcb45cf0e8e589_3079_512x512_fill_lanczos_center_3.png</url>
      <title>Artificial Intelligence</title>
      <link>http://localhost:4321/tags/artificial-intelligence/</link>
    </image>
    
    <item>
      <title>Optimizing Heat Alert Issuance with Reinforcement Learning</title>
      <link>http://localhost:4321/publication/hasdm/</link>
      <pubDate>Sun, 10 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/publication/hasdm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Revelatory Mirror of AI/ML -- On Evolution, Uncertainty, and Recourse</title>
      <link>http://localhost:4321/post/revelatory-mirror/</link>
      <pubDate>Wed, 13 Jul 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/revelatory-mirror/</guid>
      <description>&lt;p&gt;&lt;strong&gt;“Our successes and failures alike in getting these systems to do &amp;lsquo;what we want,&amp;rsquo; it turns out, offer us an unflinching, revelatory mirror.” - Brian Christian&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This summer, I joined a data science &amp;amp; society-themed book club sponsored by the Harvard Data Science Initiative. Our first book was &lt;em&gt;The Alignment Problem: Machine Learning and Human Values&lt;/em&gt; by Brian Christian (2020). It turned out to be one of my favorite nonfiction books. Someone in the group also let us know about a free virtual event, the Symposium on Interpretable and Explainable Artificial Intelligence and Machine Learning, hosted by the National Academies of Science Committee on Applied and Theoretical Statistics, which I ended up attending. (If you&amp;rsquo;re interested in this Symposium, a recording is available &lt;a href=&#34;https://www.nationalacademies.org/event/06-21-2022/interpretable-and-explainable-ai-and-machine-learning&#34;&gt;here&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;Between the reading, the Symposium, and our book club&amp;rsquo;s weekly discussions, there are a plethora of topics about which I could wax lyrical. For the sake of a bite-sized post, however, I will stick to a few overarching themes that stuck out to me: (1) the development of artificial intelligence (AI) giving us insights into our own brains, (2) the ability (or lack thereof) for individual people or minority groups to pursue justice for maltreatment at the &amp;ldquo;hands&amp;rdquo; of an algorithm, and (3) the perpetual need for humility (for both humans and AI).&lt;/p&gt;
&lt;h3 id=&#34;similarities-and-differences-between-machine-and-human-learning&#34;&gt;Similarities and Differences Between Machine and Human Learning&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;The Alignment Problem&lt;/em&gt; steps through many key developments in AI, with lots of fun anecdotes to help readers peek behind the publication curtain to see what researchers were thinking and struggling with. This progression really helped me to see why, for instance, animals (including us) require both extrinsic and intrinsic motivations (e.g. hunger and curiosity) to thrive in a variety of environments. I&amp;rsquo;m also now able to articulate both the benefits and potential consequences of imitation: you can learn things that would otherwise be really difficult to explain or craft incentives for, but it may not work out if you attempt to copy something that you&amp;rsquo;re not equipped to do and/or don&amp;rsquo;t understand the motivations for. Also, it&amp;rsquo;s hard to ever surpass the teacher if you&amp;rsquo;re just trying to match them.&lt;/p&gt;
&lt;p&gt;In retrospect, I see a lot of parallels to my personal experience learning to play the clarinet, which is by far my most structured extended learning experience. When you start out, you cling to the rules (of instrument technique and music in general) for dear life in an attempt to get off the ground: you try to do exactly what your teacher tells you, use a metronome to keep time and a tuner to check pitch. Eventually, it helps to have less of this internal rigidity and to start paying more attention to what feels right, trying to follow the &amp;ldquo;flow&amp;rdquo; of the music. Throughout this process, having a teacher to imitate and/or listening to recordings of other people playing can help you progress by leaps and bounds. At the same time, you likely won&amp;rsquo;t be able to play a piece as quickly or with as great a dynamic range as your teacher, and it will often sound worse if you try to mimic them in these ways right off the bat.&lt;/p&gt;
&lt;p&gt;Another cool development in AI has been the emergence of techniques to train AI against themselves, which I eventually recognized as a version of my clarinet teacher&amp;rsquo;s motto that the goal of taking lessons was to become my own best teacher (either in real-time or by recording and then listening to myself). A final memorable example of AI-human similarities in The Alignment Problem detailed how the development of an algorithm that allowed a computer to learn from changes in its expectation about the future (termed Temporal Difference Learning) ended up closely describing how our brains produce dopamine (a &amp;ldquo;happy hormone&amp;rdquo;), which was a huge breakthrough for neuroscience.&lt;/p&gt;
&lt;p&gt;In addition to all these similarities, there are also many ways that machine and human learning differ. For instance, &amp;ldquo;saliency&amp;rdquo; methods have shown that when some AI are trying to classify images as dogs or not-dogs, they are often looking at features like the blurriness of the outline of the object/subject in the middle of the image, as opposed to humans tending to look at the overall shape. This causes problems if we want to be able to interpret the output from algorithms: forcing computers to use our ontologies (language/thinking structure) might facilitate our understanding but prevent superhuman inference and/or be stymied by competing human ontologies**.&lt;/p&gt;
&lt;p&gt;Further, regarding the earlier point about Temporal Difference Learning: although encountering observations that conflict with our mental models offers great learning opportunities, people often resist changing our opinions, sometimes even ignoring new data instead of doing the hard cognitive work of updating our expectations. In this way, we probably don&amp;rsquo;t want AI to be too similar to human cognitive behavior. On the flip side, while certain strategies developed for guiding the behavior of AI might be applicable to humans, they may not necessarily be good for us. For example, in the book club we discussed how gamifying various aspects of our lives can in some cases be helpful (e.g. rewarding ourselves for getting work done) but in other cases are harmful (e.g. becoming addicted to video games because they are more immediately satisfying than real life).&lt;/p&gt;
&lt;h3 id=&#34;actionable-recourse-for-algorithmic-injustice&#34;&gt;Actionable Recourse for Algorithmic Injustice&lt;/h3&gt;
&lt;p&gt;Over the last six years, there has been an explosion of research and discussion about algorithmic bias against certain groups of people. Before reading The Alignment Problem, I actually hadn&amp;rsquo;t realized how young this field is because I started college in 2016, so I&amp;rsquo;ve heard about it my whole academic career. Now, in 2022, it seems like there are hundreds of different definitions of algorithmic fairness, along with mathematical theory showing that you can&amp;rsquo;t obtain them all at the same time, except possibly under very specific (unrealistic) conditions.&lt;/p&gt;
&lt;p&gt;One of the really interesting issues that my partner (who just graduated from law school) and I were talking about, and that then came up in the Symposium, is the lack of legal infrastructure for dealing with decisions made (or strongly informed) by algorithms. For instance, the US legal system currently allows appeal for decisions made by a biased jury, but not by a biased algorithm. Financial institutions are somewhat more regulated in how they may use models to inform lending decisions. However, as Patrick Hall (a professor at George Washington University) noted: even if, for instance, people are entitled to some notice of a few key features that factored into why they were denied a loan, this may be wildly insufficient for explaining an individual prediction from a (potentially proprietary) high-dimensional, nonlinear machine learning model that was designed to make the company profits when the results are averaged across all their clients. This is to say, we have a long way to go in truly providing individuals with actionable recourse against algorithms. If you&amp;rsquo;re interested in learning more on this topic, I&amp;rsquo;m linking some resources at the end of this post.&lt;/p&gt;
&lt;h3 id=&#34;cultivating-uncertainty&#34;&gt;Cultivating Uncertainty(?)&lt;/h3&gt;
&lt;p&gt;During the panel discussion at the Symposium, a common scientific concept was raised: the more independent teams/models arrive at the same conclusion, the more reason to trust in that conclusion. This intuition motivates a general technique that researchers have started using to quantify the uncertainty of AI: generate a lot of results from different models or by perturbing the same model in different ways (e.g. cutting off different parts of the model from contributing to each prediction), and see how similar all the results are to each other. More spread indicates more uncertainty in the model prediction. (Side thought: time will tell whether quantification of model uncertainty ends up playing a role in people&amp;rsquo;s recourse from AI-based decisions.)&lt;/p&gt;
&lt;p&gt;However, this approach really only works if all the models / sub-models are sufficiently distinct or independent of one another. In &lt;em&gt;The Alignment Problem&lt;/em&gt;, Christian describes how the Effective Altruism movement (see &amp;ldquo;On Having an Impact, Part 1&amp;rdquo;) struggles with maintaining moral uncertainty (i.e. for what global causes to focus attention and resources on) while simultaneously distributing educational resources and encouraging discussion between people all around the world. Similarly, while the push for open science / reproducibility and transparent decision making has many benefits, including increased methodological scrutiny and less duplicated work, it seems that there may be some downsides to encouraging everyone to do things the exact same way, using the exact same tools.&lt;/p&gt;
&lt;p&gt;In data science, for instance, using the same packages in R or Python streamlines analyses, but could also potentially produce less evidence across multiple studies on the same topic because the work is not being done independently. This would of course be most concerning if a commonly-used package had a mistake in it, but even if the methods were all correct, the convenience of using a ready-made package might result in the community missing out on other ways to think about a problem that would provide different insights. Similarly, if the vast majority of publicly available datasets are biased towards white males, a consensus across these datasets is not necessarily a better representation of the population overall. (Note: I&amp;rsquo;m certainly not arguing for a return to siloed and/or poorly documented workflows &amp;ndash; just trying to articulate a point that I haven&amp;rsquo;t heard mentioned as much compared to the huge push for open science.)&lt;/p&gt;
&lt;p&gt;Even if we thought we had a robust uncertainty estimate, maintaining a heightened sense of humility is especially important for AI because we want to avoid a situation in which the machines are no longer amenable to human input. Private and/or public sector creators of AI systems might also want machines to be even more careful than human actors would be for many practical applications, as they will often be judged more harshly by the public (and potentially, in the future, by the law). For an example of this, I&amp;rsquo;ll draw from &lt;em&gt;The Alignment Problem&lt;/em&gt;&amp;rsquo;s description of the relationship between uncertainty and impact: if a self-driving car is unsure whether there&amp;rsquo;s a person in the road in front of it, then the best course of action is to slow down so as to minimize adverse outcomes.&lt;/p&gt;
&lt;p&gt;To summarize: on her concluding presentation slide at the Symposium, Been Kim (a research scientist at Google Brain) wrote, &amp;ldquo;One of the biggest challenges [in interpreting/explaining AI] is to resist building trust and be skeptical.&amp;rdquo; I believe she meant this in the context of humans interpreting a single model, but the sentiment could also easily be applied to humans interpreting ensembles of dependent models and/or data, as well as to AI continuing to make decisions after being deployed for a while.&lt;/p&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;I am far from being an expert in these topics, but I hope that my rambles have left you with some interesting thoughts and/or whetted your appetite to go learn more from people who&amp;rsquo;ve studied these issues in depth. My overarching takeaways from all of this are that (1) there&amp;rsquo;s a lot of interesting work being done in this field, (2) there&amp;rsquo;s a lot more to do to ensure AI systems live up to our ideals, (3) we have the opportunity to learn a lot about ourselves along the way, and (4) as whizzy as our tech gets, humanity will need to stay humble to avoid losing ourselves.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;** Here is a 2022 &lt;a href=&#34;https://news.mit.edu/2022/humans-understand-robots-psychology-0302?mc_cid=c2c577fee8&amp;amp;mc_eid=eb939aedf9&#34;&gt;article that describes how researchers are using team psychology to help humans learn to collaborate with robots faster and more effectively&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here are some interesting resources regarding AI standards / regulation / actionable recourse against AI: &lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/frai.2021.695301/full&#34;&gt;A United States Fair Lending Perspective on Machine Learning&lt;/a&gt; (2021); &lt;a href=&#34;https://pursuit.unimelb.edu.au/articles/challenging-decisions-made-by-algorithm&#34;&gt;Challenging Decisions Made by Algorithms&lt;/a&gt; (2021); &lt;a href=&#34;https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1270.pdf&#34;&gt;NIST: Towards a Standard for Identifying and Managing Bias in Artificial Intelligence&lt;/a&gt; (2022); &lt;a href=&#34;https://incidentdatabase.ai&#34;&gt;https://incidentdatabase.ai&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Others&#39; Ideas that Resonate</title>
      <link>http://localhost:4321/project/ideas_resonate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/project/ideas_resonate/</guid>
      <description>&lt;p&gt;Here are lists of articles, podcasts, videos, etc. that I intend to continuously update as I come across well-explained, powerful ideas that relate directly to my field(s) of interest. For the most part, these resources are more conversational than technical, and are meant to inspire as well as inform. Also see my list of recommended nonfiction books, which spans a larger range of topics.&lt;/p&gt;
&lt;h3 id=&#34;environmental-health&#34;&gt;Environmental Health&lt;/h3&gt;
&lt;p&gt;Articles:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Assessing the health burden from air pollution &amp;ndash; Science perspective, 2024&lt;/li&gt;
&lt;li&gt;StoryMap: &amp;ldquo;Air Pollution and Health: The Global Impacts of Fossil Fuels&amp;rdquo; &amp;ndash; State of Global Air, 2023&lt;/li&gt;
&lt;li&gt;Engaging Socially Vulnerable Communities and Communicating About Climate Change &amp;ndash; NASEM, 2023&lt;/li&gt;
&lt;li&gt;Leveraging Air Quality Research Advancements to Assess and Address Exposure Inequities &amp;ndash; Goldman, 2023. Environmental Health Perspectives&lt;/li&gt;
&lt;li&gt;Are “safe” exposure levels really “safe”? &amp;ndash; Program on Reproductive Health and the Environment, 2023&lt;/li&gt;
&lt;li&gt;Western lawmakers spot opening in smoke crisis &amp;ndash; Axios, 2023&lt;/li&gt;
&lt;li&gt;A conversation on air pollution in the United States &amp;ndash; Nature Geoscience, 2023&lt;/li&gt;
&lt;li&gt;Naming civic health in environmental justice discourse: The Jackson water crisis &amp;ndash; Nwanaji-Enwerem &amp;amp; Casey, 2022&lt;/li&gt;
&lt;li&gt;Flipping the Climate Debate from Costs to Benefits &amp;ndash; Anenberg, 2021. Environmental Health News&lt;/li&gt;
&lt;li&gt;Tackling Air Pollution and Climate Justice Together &amp;ndash; Moses &amp;amp; Roy, 2021. Environmental Defense Fund&lt;/li&gt;
&lt;li&gt;Closing the Data Gap: A Cost-Effective Way to Improve [Global] Air Quality &amp;ndash; Hasenkopf, 2021. World Economic Forum&lt;/li&gt;
&lt;li&gt;Air Pollution Diminishes Future Generations&amp;rsquo; Economic Opportunities &amp;ndash; Voorheis, 2021. US Census&lt;/li&gt;
&lt;li&gt;Using Feedback to Improve Accountability in Global Environmental Health and Engineering &amp;ndash; Thomas &amp;amp; Brown, 2020&lt;/li&gt;
&lt;li&gt;Is AI revolutionizing environmental health? (Simply Statistics &amp;ndash; Peng, 2019). In general, this blog has a lot of cool posts for data scientists and educators.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Videos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Environmental Health Disparities - An Overview (Harvard Chan NIEHS Center for Environmental Health) &amp;ndash; Mendonça, 2021&lt;/li&gt;
&lt;li&gt;ClimateBits: Unhealthy Air &amp;ndash; 2019.&lt;/li&gt;
&lt;li&gt;What is Environmental Health? &amp;ndash; YouTube, NYU School of Global Public Health, 2019&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Podcasts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Agents of Change in Environmental Justice &amp;ndash; podcast featuring emerging leaders from historically excluded backgrounds in science and academia who are reimagining solutions for a just and healthy planet&lt;/li&gt;
&lt;li&gt;This is Your Brain on Pollution &amp;ndash; Freakonomics podcast, 2021&lt;/li&gt;
&lt;li&gt;Using Data Science to Study Air Pollution&amp;rsquo;s Effect on Covid-19 (Women in Data Science podcast) &amp;ndash; Dominici &amp;amp; Nethery, 2020&lt;/li&gt;
&lt;li&gt;Atmospheric Tales &amp;ndash; podcast discussing global environmental health, 2020 onwards&lt;/li&gt;
&lt;li&gt;Decolonizing Air Pollution Science &amp;ndash; deSouza, 2020 / MIT Community Innovators Lab (CoLab) Radio (overall archive). Stories and reflections on the interplay of science and technology with community development and planning.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Feature-length / series:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How air pollution impacts our brains, 2024&lt;/li&gt;
&lt;li&gt;NASEM Climate Crossroads Summit 2023 (video playlist)&lt;/li&gt;
&lt;li&gt;NASEM Workshop on Communities, Climate Change, and Health Equity – Lessons Learned in Addressing Inequities in Heat-Related Climate Change Impacts (2023)&lt;/li&gt;
&lt;li&gt;HSPH Climate, Health, &amp;amp; Equity Symposium 2023&lt;/li&gt;
&lt;li&gt;Health Effects Institute (HEI) Science on the 7th &amp;ndash; series discussing current critical topics in air quality and global health&lt;/li&gt;
&lt;li&gt;Clearing the Air: Why Now Is the Time to Tackle Global Air Pollution &amp;ndash; HSPH + WHO + CNN, 2021&lt;/li&gt;
&lt;li&gt;Taking the Temperature: Your Health, Our Climate, and What We Can Do About It &amp;ndash; Mazumder, 2021. Harvard Science in the News series.&lt;/li&gt;
&lt;li&gt;The Story of Plastic (2019). Documentary examining issues and opportunities throughout the lifecycle of plastic.&lt;/li&gt;
&lt;li&gt;Dark Waters (2019). Dramatic recreation of Rob Bilott&amp;rsquo;s case against the chemical manufacturing company DuPont after they contaminated a town with unregulated chemicals. &lt;em&gt;For more information on the environmental health science occurring behind the scenes, check out the C8 Science Panel website.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Collaborative on Health and the Environment: Climate Change and Health, 2019. Collection of videos, focusing on the topics of extreme heat, air pollution, extreme weather events, and infectious diseases.&lt;/li&gt;
&lt;li&gt;The Crowd &amp;amp; the Cloud &amp;ndash; Abdalati, 2017. Video series discussing citizen science used in environmental health applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;data-science--statistics&#34;&gt;Data Science / Statistics&lt;/h3&gt;
&lt;p&gt;Articles:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Big data in Earth science: Emerging practice and promise &amp;ndash; Science review, 2024&lt;/li&gt;
&lt;li&gt;Democratizing the future of AI R&amp;amp;D: NSF to launch National AI Research Resource pilot (2024)&lt;/li&gt;
&lt;li&gt;What Should Data Science Education Do with Large Language Models? &amp;ndash; Tu et al, 2023&lt;/li&gt;
&lt;li&gt;NASA and IBM Openly Release Geospatial AI Foundation Model for NASA Earth Observation Data (2023)&lt;/li&gt;
&lt;li&gt;Strengthening [global] public health systems through artificial intelligence &amp;ndash; HSPH, 2023&lt;/li&gt;
&lt;li&gt;Harvard Graduate Transforms High School Math Curriculum: Skew The Script, a nonprofit organization, provides free, socially-relevant math + statistics lessons to high school teachers across the country &amp;ndash; Harvard Data Science Initiative, 2023&lt;/li&gt;
&lt;li&gt;Apple Women’s Health Study advances understanding of menstrual cycles and health (2023) &amp;ndash;Example of thoughtful data collection generating insights that are highly relevant for both clinicians and policy makers.&lt;/li&gt;
&lt;li&gt;Is there a role for statistics in artificial intelligence? &amp;ndash; Friedrich et al, 2021&lt;/li&gt;
&lt;li&gt;What are the Most Important Statistical Ideas of the Past 50 Years? &amp;ndash; Gelman &amp;amp; Vehtari, 2021&lt;/li&gt;
&lt;li&gt;Artificial Intelligence&amp;rsquo;s Promise &amp;amp; Peril (2021) &amp;ndash; Harvard School of Public Health&lt;/li&gt;
&lt;li&gt;Twitter thread: best data visualizations (2021)&lt;/li&gt;
&lt;li&gt;What Are the Values of Data, Data Science, or Data Scientists? &amp;ndash; Meng, 2021. Harvard Data Science Review editorial piece.&lt;/li&gt;
&lt;li&gt;Statistics Monitors the Environment and Statistics Informs Health Policy &amp;ndash; ASA Statistical Significance Series&lt;/li&gt;
&lt;li&gt;The ASCCR Frame for Learning Essential Collaboration Skills &amp;ndash; Vance &amp;amp; Smith, 2019&lt;/li&gt;
&lt;li&gt;Career of Gertrude M. Cox &amp;ndash; NC State Library&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Podcasts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mental Health Challenges: How Can Data Science Help? &amp;ndash; Harvard Data Science Review Podcast, 2021&lt;/li&gt;
&lt;li&gt;Casual Inference podcast by Ellie Murray and Lucy D&amp;rsquo;Agostino McGowan, specifically this episode featuring Roger Peng: &amp;ldquo;Why Everyone is Excited About Causal Inference These Days&amp;rdquo; (2020)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Feature-length / series:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Shaping the Future of AI &amp;ndash; National Academies; one insightful discussion can be found in the &amp;ldquo;Navigating Ethical Dilemmas&amp;rdquo; section in this article&lt;/li&gt;
&lt;li&gt;Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead (Harvard Data Science Initiative Bias^2 seminar) &amp;ndash; Rudin, 2021&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;career--miscellaneous&#34;&gt;Career / Miscellaneous&lt;/h3&gt;
&lt;p&gt;Articles:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Human Implications of AI with Brian Christian &amp;ndash; NASEM, 2024&lt;/li&gt;
&lt;li&gt;Time to support Indigenous science &amp;ndash; Wall Kimmerer &amp;amp; Artelle, 2024&lt;/li&gt;
&lt;li&gt;How are you saving the world? &amp;ndash; Parast, 2023&lt;/li&gt;
&lt;li&gt;All the evidence-based advice we found on how to be more successful in any job &amp;ndash; Todd (80,000 Hours), 2023&lt;/li&gt;
&lt;li&gt;Field-Specific Ability Beliefs as an Explanation for Gender Differences in Academics’ Career Trajectories: Evidence From Public Profiles on ORCID.org &amp;ndash; Hannak et al, 2023&lt;/li&gt;
&lt;li&gt;Increasing Women’s Representation in STEM Fields &amp;ndash; Gray, 2023&lt;/li&gt;
&lt;li&gt;Which food is better for the planet? &amp;ndash; Washington Post Climate Lab (2023). Interactive visualizations comparing different food types over various environmental dimensions.&lt;/li&gt;
&lt;li&gt;A list of the most high-impact, evidence-based organizations fighting climate change &amp;ndash; Samuel, 2022&lt;/li&gt;
&lt;li&gt;Rethinking international development strategy—and hearing echoes of global challenges in the United States &amp;ndash; Harvard Kennedy School faculty, 2022&lt;/li&gt;
&lt;li&gt;Impostors Anonymous &amp;ndash; Witten, 2022. Helpful thoughts on impostor syndrome in academia / high-achievement communities.&lt;/li&gt;
&lt;li&gt;My experience with imposter syndrome—and how to (partly) overcome it &amp;ndash; Rodriguez (80,000 Hours), 2022&lt;/li&gt;
&lt;li&gt;Americans experience a false social reality by underestimating popular climate policy support by nearly half &amp;ndash; Sparkman, Geiger, &amp;amp; Weber, 2022.&lt;/li&gt;
&lt;li&gt;In academia, we need two types of researchers: divers and surfers &amp;ndash; Pandya, 2022&lt;/li&gt;
&lt;li&gt;Putting Mental Health in America on the Map &amp;ndash; Geraghty (ESRI), 2022&lt;/li&gt;
&lt;li&gt;How Public Health Took Part in Its Own Downfall &amp;ndash; The Atlantic, 2021&lt;/li&gt;
&lt;li&gt;Effective Altruism: An Update to Our Thinking On Climate Change &amp;ndash; Ackva, 2020&lt;/li&gt;
&lt;li&gt;Five Ways To Prioritize Better &amp;ndash; Bye, 2020&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Videos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;200 Years in 4 Minutes &amp;ndash; Rosling, 2017. Global relationships between lifespan and income over time, across countries. Note: there are many videos of Rosling presenting other cool data visualizations&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Podcasts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Degrees of Freedom &amp;ndash; Hasenkopf, 2020. Exploring many different ways that scientists can have positive impact through their careers.&lt;/li&gt;
&lt;li&gt;AAAS STPF Sci on the Fly. A variety of science and policy topics, written by scientists working in the federal government.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Feature-length / series:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Harvard School of Public Health YouTube Channel&lt;/li&gt;
&lt;li&gt;Science x Policy Crossroads: Pathways toward Health Policy &amp;ndash; ten people&amp;rsquo;s stories illustrating different careers at the intersection of science and policy, put together by the National Science Policy Network&lt;/li&gt;
&lt;li&gt;AAAS Voices: Countering Science Misinformation (2021)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>

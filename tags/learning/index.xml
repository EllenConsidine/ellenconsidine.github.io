<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Learning | Ellen Considine</title>
    <link>https://EllenConsidine.github.io/tags/learning/</link>
      <atom:link href="https://EllenConsidine.github.io/tags/learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Learning</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Fri, 03 Feb 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://EllenConsidine.github.io/media/icon_hu68170e94a17a2a43d6dcb45cf0e8e589_3079_512x512_fill_lanczos_center_3.png</url>
      <title>Learning</title>
      <link>https://EllenConsidine.github.io/tags/learning/</link>
    </image>
    
    <item>
      <title>Statistical Consulting and Learning-Centered Teaching</title>
      <link>https://EllenConsidine.github.io/post/stat-collab-teaching/</link>
      <pubDate>Fri, 03 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://EllenConsidine.github.io/post/stat-collab-teaching/</guid>
      <description>&lt;p&gt;This academic year, I am a pedagogy fellow (PF) at the Harvard School of Public Health (HSPH). Instead of being a TF/TA for the Biostatistics Department (which typically involves grading, teaching labs and holding office hours for students), I work on developing curriculum and encouraging evidence-based teaching practices, both within my department and for the school overall. For HSPH-wide projects, I collaborate with PFs from other departments. In this post, I&amp;rsquo;ll focus on insights inspired by my department-specific project as well as by a course on teaching that all the PFs took last semester.&lt;/p&gt;
&lt;p&gt;For context: in spring 2020, I really enjoyed taking a course at CU on statistical collaboration. (The Professor, Eric Vance, used the term collaboration as opposed to consulting to emphasize the importance of consistent, two-way communication between statisticians and domain experts.) When I heard that my current department was looking for a PF to redesign our PhD-level statistical consulting course, I applied immediately.&lt;/p&gt;
&lt;p&gt;Beyond digging into statistical consulting, being a PF has been a great opportunity to think deeply about whether I want teaching to be a core part of my future career. The upshot on that front is that I&amp;rsquo;m still making up my mind. However, many of the ideas I&amp;rsquo;ve been synthesizing are broadly applicable on both individual and systemic levels, sufficiently so that they merit a post. The three main topics I will discuss are&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Evidence-based tools in education&lt;/li&gt;
&lt;li&gt;Key skills common to teaching and consulting / collaboration&lt;/li&gt;
&lt;li&gt;Broad musings on modern education&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;evidence-based-tools-in-education&#34;&gt;Evidence-Based Tools in Education&lt;/h3&gt;
&lt;p&gt;At the start of Teaching 100 (the course the PFs took last semester), we read the book &lt;em&gt;Make it Stick: The Science of Successful Learning&lt;/em&gt;. We then proceeded to have eight weeks of discussions, led by instructor Tari Tan, about this material and more. During the class and in the months since, a few concepts in particular have stuck out to me.&lt;/p&gt;
&lt;p&gt;The number one takeaway from &lt;em&gt;Make it Stick&lt;/em&gt; is that retrieval practice is critical for long-term learning. Retrieval practice is honestly testing your knowledge (as opposed to re-reading material), ideally waiting long enough between practices that you start to forget. Learning both concepts and skills this way is intentionally challenging, but has been shown over and over again to yield better educational outcomes than traditional studying. I was introduced to retrieval practice through studying music (my teacher would have me do practice performances where I couldn&amp;rsquo;t look at the sheet music), but looking back on my academic experience, I think retrieval practice is still under-utilized in most traditional classroom settings.&lt;/p&gt;
&lt;p&gt;A related concept is one that many people nowadays are familiar with: the power of cultivating a growth mindset. Despite the proliferation of this idea, STEM education (and math in particular) struggles with deeply-rooted notions that some people are destined to flourish and others are doomed to flail. My personal experience in math classes from middle school through grad school is that it can be really hard to prevent unsuccessful attempts from harming your perception of your overall competence (see &amp;ldquo;Potent Challenges&amp;rdquo; in a past blog post). Taking the time to think deeply about these issues in Teaching 100, I began to wonder whether centering tools like retrieval practice in classrooms (from an early age) and explicitly reinforcing the notion that all meaningful learning is difficult (and therefore must be undertaken iteratively) might boost many people&amp;rsquo;s development of growth mindsets. (Note: it&amp;rsquo;s possible that such a transition in standard educational practice is already somewhat under way, given what my mom said about her experience getting a teaching license several years ago. After all, it&amp;rsquo;s been two decades since I started kindergarten.)&lt;/p&gt;
&lt;p&gt;A third aspect of Teaching 100 that I found especially compelling was active learning. The definition of &amp;lsquo;active learning&amp;rsquo; is very broad, but generally refers to educational activities with more student engagement than just listening to an instructor lecture for 60-90 minutes. While I previously had a somewhat lukewarm perception of the push for active learning (especially in higher education), in Teaching 100 we were presented with evidence that in addition to helping a class of students overall, active learning tends to disproportionately benefit students from marginalized racial and socioeconomic backgrounds. The more I reflect on this, the more it makes sense to me that increasing interaction between students and instructors, encouraging and guiding metacognition (self-reflection), and providing some additional course structure can help narrow educational achievement gaps between those who have received more academic / professional mentorship and those who haven&amp;rsquo;t. Deploying active learning strategies to address inequity in the classroom complements my discussion in &amp;ldquo;Reflections on Privilege in Higher Education&amp;rdquo;.&lt;/p&gt;
&lt;h3 id=&#34;key-skills-common-to-teaching-and-consulting&#34;&gt;Key Skills Common to Teaching and Consulting&lt;/h3&gt;
&lt;p&gt;Given my field of study, my thinking about consulting / collaboration is inevitably tied to statistics / data science. However, my sense is that many of these ideas apply much more generally. In fact, reflecting on my experience working to create curriculum for the statistical consulting course, I notice a lot of similarities between best practices taught in Statistical Collaboration and in Teaching 100. I share the most prominent here.&lt;/p&gt;
&lt;p&gt;An easy place to start is the importance of &lt;strong&gt;cultivating a welcoming environment&lt;/strong&gt; where people feel respected and are comfortable asking questions and voicing concerns. This is a core tenet both of learning-centered teaching and of building a productive working relationship with a domain expert / statistical consulting client. I won&amp;rsquo;t go into details here, but I believe in the power of both verbal and non-verbal cues!&lt;/p&gt;
&lt;p&gt;Two related practices are an immediate focus on identifying each party&amp;rsquo;s goals / motivations and periodically &lt;strong&gt;ensuring that the work is in alignment with the goals&lt;/strong&gt;. As a statistician, this includes inquiring about what the intended product(s) and audience of a domain expert&amp;rsquo;s study will be, and highlighting any mismatches (as they arise) between the question of interest and available data / methods. As a teacher, this includes clearly articulating the course objectives and learning goals (e.g. in a syllabus) and helping students to align their learning experience with their educational / career goals, for instance through intermittent metacognition / reflection assignments. In both settings, &lt;strong&gt;writing down a clear plan&lt;/strong&gt; at the beginning of the partnership, explicitly laying out expectations and timelines for both productivity and communication between parties, is extremely helpful for getting everyone on the same page and making progress towards both parties&amp;rsquo; goals.&lt;/p&gt;
&lt;p&gt;In addition to clarifying overall goals, a communication strategy that resonates a lot with me for both settings is &lt;strong&gt;leading with intent&lt;/strong&gt;. Whether asking a domain expert a question about their data or assigning students a task, giving a brief explanation of why it matters / what you&amp;rsquo;re looking for often yields better outcomes (e.g., information or students&amp;rsquo; work).&lt;/p&gt;
&lt;p&gt;Especially in statistical consulting settings, leading with intent can be used to gracefully assess a domain expert&amp;rsquo;s knowledge of statistics / coding, which then facilitates giving explanations and advice tailored to their level of understanding. In teaching, this is referred to as the &lt;strong&gt;zone of proximal development&lt;/strong&gt;: information and skills that a learner can realistically assimilate into their mental framework in the near term, given what they already know. When starting work in a new area of application, a statistician also must learn on the fly. A technique that we practiced frequently in Statistical Collaboration was to pause after a domain expert&amp;rsquo;s explanation and, instead of immediately moving on to the next thing, &lt;strong&gt;summarize back our understanding&lt;/strong&gt; of what they said. This helps to catch and rectify misconceptions of both the goals and technical details of a project early on, before it becomes burdensome to correct course. Similarly, asking a domain expert or a student to explain out loud (or in writing) what they took away from one of my explanations is a great way to check whether they understood.&lt;/p&gt;
&lt;p&gt;In general, a big takeaway for me from both Statistical Collaboration and Teaching 100 is that working with other people is so much more effective and enjoyable when one is familiar with well-tested strategies for doing so. Of course, there is always a chance of unforeseen circumstances that require ad hoc responses. But from my experience and countless stories from others, it seems like a large amount of professional stress is predictable and avoided by consistently implementing existing strategies.&lt;/p&gt;
&lt;h3 id=&#34;broad-musings-about-modern-education&#34;&gt;Broad Musings about Modern Education&lt;/h3&gt;
&lt;p&gt;Continuing on the theme of interpersonal skills, in Teaching 100 our main project was to design a syllabus for a course, and I noticed that many of us in the class chose topics meant to address the &amp;ldquo;soft side&amp;rdquo; of our technical disciplines, such as statisticians working with domain experts or doctors working with patients. Traditionally, at least in STEM, there hasn&amp;rsquo;t been much formal training along these lines. Skills in professional communication and/or teaching are often undervalued (e.g. in academic tenure review) and it is mostly expected that people either mimic their advisors / managers or figure it out on their own through trial and error. Especially given the self-selection of many people who have natural affinity for social interaction into other fields, it makes sense that classroom educators might try to fill this gap more systematically.&lt;/p&gt;
&lt;p&gt;However, as medicine in particular seems to have known for a long time, professional interpersonal skills are very difficult to learn in only a classroom setting. But outside the classroom, it is harder for educators to actually observe and guide their students&amp;rsquo; improvement. If the instructor of a statistical consulting course doesn&amp;rsquo;t have the time / compensation to sit through client meetings with every one of their students, then they (and their students) must have a certain amount of confidence in the students&amp;rsquo; skills to venture forth and report back. This becomes less realistic the more students are in the class and/or the more variation in the students&amp;rsquo; skill levels. I have yet to arrive at a conclusion about how best to deal with this tension.&lt;/p&gt;
&lt;p&gt;Recent events appear to have made interpersonal skills more important than ever. The emergence of tools such as ChatGPT largely level the playing field for many writing-centric tasks, including composition of cover letters. My suspicion is that live interviews and collaboration skills will become ever more important in hiring for professional positions, in addition to promotions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Revelatory Mirror of AI/ML -- On Evolution, Uncertainty, and Recourse</title>
      <link>https://EllenConsidine.github.io/post/revelatory-mirror/</link>
      <pubDate>Wed, 13 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://EllenConsidine.github.io/post/revelatory-mirror/</guid>
      <description>&lt;p&gt;&lt;strong&gt;“Our successes and failures alike in getting these systems to do &amp;lsquo;what we want,&amp;rsquo; it turns out, offer us an unflinching, revelatory mirror.” - Brian Christian&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This summer, I joined a data science &amp;amp; society-themed book club sponsored by the Harvard Data Science Initiative. Our first book was &lt;em&gt;The Alignment Problem: Machine Learning and Human Values&lt;/em&gt; by Brian Christian (2020). It turned out to be one of my favorite nonfiction books. Someone in the group also let us know about a free virtual event, the Symposium on Interpretable and Explainable Artificial Intelligence and Machine Learning, hosted by the National Academies of Science Committee on Applied and Theoretical Statistics, which I ended up attending. (If you&amp;rsquo;re interested in this Symposium, a recording is available &lt;a href=&#34;https://www.nationalacademies.org/event/06-21-2022/interpretable-and-explainable-ai-and-machine-learning&#34;&gt;here&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;Between the reading, the Symposium, and our book club&amp;rsquo;s weekly discussions, there are a plethora of topics about which I could wax lyrical. For the sake of a bite-sized post, however, I will stick to a few overarching themes that stuck out to me: (1) the development of artificial intelligence (AI) giving us insights into our own brains, (2) the ability (or lack thereof) for individual people or minority groups to pursue justice for maltreatment at the &amp;ldquo;hands&amp;rdquo; of an algorithm, and (3) the perpetual need for humility (for both humans and AI).&lt;/p&gt;
&lt;h3 id=&#34;similarities-and-differences-between-machine-and-human-learning&#34;&gt;Similarities and Differences Between Machine and Human Learning&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;The Alignment Problem&lt;/em&gt; steps through many key developments in AI, with lots of fun anecdotes to help readers peek behind the publication curtain to see what researchers were thinking and struggling with. This progression really helped me to see why, for instance, animals (including us) require both extrinsic and intrinsic motivations (e.g. hunger and curiosity) to thrive in a variety of environments. I&amp;rsquo;m also now able to articulate both the benefits and potential consequences of imitation: you can learn things that would otherwise be really difficult to explain or craft incentives for, but it may not work out if you attempt to copy something that you&amp;rsquo;re not equipped to do and/or don&amp;rsquo;t understand the motivations for. Also, it&amp;rsquo;s hard to ever surpass the teacher if you&amp;rsquo;re just trying to match them.&lt;/p&gt;
&lt;p&gt;In retrospect, I see a lot of parallels to my personal experience learning to play the clarinet, which is by far my most structured extended learning experience. When you start out, you cling to the rules (of instrument technique and music in general) for dear life in an attempt to get off the ground: you try to do exactly what your teacher tells you, use a metronome to keep time and a tuner to check pitch. Eventually, it helps to have less of this internal rigidity and to start paying more attention to what feels right, trying to follow the &amp;ldquo;flow&amp;rdquo; of the music. Throughout this process, having a teacher to imitate and/or listening to recordings of other people playing can help you progress by leaps and bounds. At the same time, you likely won&amp;rsquo;t be able to play a piece as quickly or with as great a dynamic range as your teacher, and it will often sound worse if you try to mimic them in these ways right off the bat.&lt;/p&gt;
&lt;p&gt;Another cool development in AI has been the emergence of techniques to train AI against themselves, which I eventually recognized as a version of my clarinet teacher&amp;rsquo;s motto that the goal of taking lessons was to become my own best teacher (either in real-time or by recording and then listening to myself). A final memorable example of AI-human similarities in The Alignment Problem detailed how the development of an algorithm that allowed a computer to learn from changes in its expectation about the future (termed Temporal Difference Learning) ended up closely describing how our brains produce dopamine (a &amp;ldquo;happy hormone&amp;rdquo;), which was a huge breakthrough for neuroscience.&lt;/p&gt;
&lt;p&gt;In addition to all these similarities, there are also many ways that machine and human learning differ. For instance, &amp;ldquo;saliency&amp;rdquo; methods have shown that when some AI are trying to classify images as dogs or not-dogs, they are often looking at features like the blurriness of the outline of the object/subject in the middle of the image, as opposed to humans tending to look at the overall shape. This causes problems if we want to be able to interpret the output from algorithms: forcing computers to use our ontologies (language/thinking structure) might facilitate our understanding but prevent superhuman inference and/or be stymied by competing human ontologies**.&lt;/p&gt;
&lt;p&gt;Further, regarding the earlier point about Temporal Difference Learning: although encountering observations that conflict with our mental models offers great learning opportunities, people often resist changing our opinions, sometimes even ignoring new data instead of doing the hard cognitive work of updating our expectations. In this way, we probably don&amp;rsquo;t want AI to be too similar to human cognitive behavior. On the flip side, while certain strategies developed for guiding the behavior of AI might be applicable to humans, they may not necessarily be good for us. For example, in the book club we discussed how gamifying various aspects of our lives can in some cases be helpful (e.g. rewarding ourselves for getting work done) but in other cases are harmful (e.g. becoming addicted to video games because they are more immediately satisfying than real life).&lt;/p&gt;
&lt;h3 id=&#34;actionable-recourse-for-algorithmic-injustice&#34;&gt;Actionable Recourse for Algorithmic Injustice&lt;/h3&gt;
&lt;p&gt;Over the last six years, there has been an explosion of research and discussion about algorithmic bias against certain groups of people. Before reading The Alignment Problem, I actually hadn&amp;rsquo;t realized how young this field is because I started college in 2016, so I&amp;rsquo;ve heard about it my whole academic career. Now, in 2022, it seems like there are hundreds of different definitions of algorithmic fairness, along with mathematical theory showing that you can&amp;rsquo;t obtain them all at the same time, except possibly under very specific (unrealistic) conditions.&lt;/p&gt;
&lt;p&gt;One of the really interesting issues that my partner (who just graduated from law school) and I were talking about, and that then came up in the Symposium, is the lack of legal infrastructure for dealing with decisions made (or strongly informed) by algorithms. For instance, the US legal system currently allows appeal for decisions made by a biased jury, but not by a biased algorithm. Financial institutions are somewhat more regulated in how they may use models to inform lending decisions. However, as Patrick Hall (a professor at George Washington University) noted: even if, for instance, people are entitled to some notice of a few key features that factored into why they were denied a loan, this may be wildly insufficient for explaining an individual prediction from a (potentially proprietary) high-dimensional, nonlinear machine learning model that was designed to make the company profits when the results are averaged across all their clients. This is to say, we have a long way to go in truly providing individuals with actionable recourse against algorithms. If you&amp;rsquo;re interested in learning more on this topic, I&amp;rsquo;m linking some resources at the end of this post.&lt;/p&gt;
&lt;h3 id=&#34;cultivating-uncertainty&#34;&gt;Cultivating Uncertainty(?)&lt;/h3&gt;
&lt;p&gt;During the panel discussion at the Symposium, a common scientific concept was raised: the more independent teams/models arrive at the same conclusion, the more reason to trust in that conclusion. This intuition motivates a general technique that researchers have started using to quantify the uncertainty of AI: generate a lot of results from different models or by perturbing the same model in different ways (e.g. cutting off different parts of the model from contributing to each prediction), and see how similar all the results are to each other. More spread indicates more uncertainty in the model prediction. (Side thought: time will tell whether quantification of model uncertainty ends up playing a role in people&amp;rsquo;s recourse from AI-based decisions.)&lt;/p&gt;
&lt;p&gt;However, this approach really only works if all the models / sub-models are sufficiently distinct or independent of one another. In &lt;em&gt;The Alignment Problem&lt;/em&gt;, Christian describes how the Effective Altruism movement (see &amp;ldquo;On Having an Impact, Part 1&amp;rdquo;) struggles with maintaining moral uncertainty (i.e. for what global causes to focus attention and resources on) while simultaneously distributing educational resources and encouraging discussion between people all around the world. Similarly, while the push for open science / reproducibility and transparent decision making has many benefits, including increased methodological scrutiny and less duplicated work, it seems that there may be some downsides to encouraging everyone to do things the exact same way, using the exact same tools.&lt;/p&gt;
&lt;p&gt;In data science, for instance, using the same packages in R or Python streamlines analyses, but could also potentially produce less evidence across multiple studies on the same topic because the work is not being done independently. This would of course be most concerning if a commonly-used package had a mistake in it, but even if the methods were all correct, the convenience of using a ready-made package might result in the community missing out on other ways to think about a problem that would provide different insights. Similarly, if the vast majority of publicly available datasets are biased towards white males, a consensus across these datasets is not necessarily a better representation of the population overall. (Note: I&amp;rsquo;m certainly not arguing for a return to siloed and/or poorly documented workflows &amp;ndash; just trying to articulate a point that I haven&amp;rsquo;t heard mentioned as much compared to the huge push for open science.)&lt;/p&gt;
&lt;p&gt;Even if we thought we had a robust uncertainty estimate, maintaining a heightened sense of humility is especially important for AI because we want to avoid a situation in which the machines are no longer amenable to human input. Private and/or public sector creators of AI systems might also want machines to be even more careful than human actors would be for many practical applications, as they will often be judged more harshly by the public (and potentially, in the future, by the law). For an example of this, I&amp;rsquo;ll draw from &lt;em&gt;The Alignment Problem&lt;/em&gt;&amp;rsquo;s description of the relationship between uncertainty and impact: if a self-driving car is unsure whether there&amp;rsquo;s a person in the road in front of it, then the best course of action is to slow down so as to minimize adverse outcomes.&lt;/p&gt;
&lt;p&gt;To summarize: on her concluding presentation slide at the Symposium, Been Kim (a research scientist at Google Brain) wrote, &amp;ldquo;One of the biggest challenges [in interpreting/explaining AI] is to resist building trust and be skeptical.&amp;rdquo; I believe she meant this in the context of humans interpreting a single model, but the sentiment could also easily be applied to humans interpreting ensembles of dependent models and/or data, as well as to AI continuing to make decisions after being deployed for a while.&lt;/p&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;I am far from being an expert in these topics, but I hope that my rambles have left you with some interesting thoughts and/or whetted your appetite to go learn more from people who&amp;rsquo;ve studied these issues in depth. My overarching takeaways from all of this are that (1) there&amp;rsquo;s a lot of interesting work being done in this field, (2) there&amp;rsquo;s a lot more to do to ensure AI systems live up to our ideals, (3) we have the opportunity to learn a lot about ourselves along the way, and (4) as whizzy as our tech gets, humanity will need to stay humble to avoid losing ourselves.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;** Here is a 2022 &lt;a href=&#34;https://news.mit.edu/2022/humans-understand-robots-psychology-0302?mc_cid=c2c577fee8&amp;amp;mc_eid=eb939aedf9&#34;&gt;article that describes how researchers are using team psychology to help humans learn to collaborate with robots faster and more effectively&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here are some interesting resources regarding AI standards / regulation / actionable recourse against AI: &lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/frai.2021.695301/full&#34;&gt;A United States Fair Lending Perspective on Machine Learning&lt;/a&gt; (2021); &lt;a href=&#34;https://pursuit.unimelb.edu.au/articles/challenging-decisions-made-by-algorithm&#34;&gt;Challenging Decisions Made by Algorithms&lt;/a&gt; (2021); &lt;a href=&#34;https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1270.pdf&#34;&gt;NIST: Towards a Standard for Identifying and Managing Bias in Artificial Intelligence&lt;/a&gt; (2022); &lt;a href=&#34;https://incidentdatabase.ai&#34;&gt;https://incidentdatabase.ai&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
